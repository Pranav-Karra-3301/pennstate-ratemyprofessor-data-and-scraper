name: Update Penn State Professor Data

on:
  schedule:
    # Run every 4 months: January 1st, May 1st, September 1st
    # Format: minute hour day month day-of-week
    - cron: '0 6 1 1 *'    # January 1st at 6 AM UTC (Spring semester)
    - cron: '0 6 1 5 *'    # May 1st at 6 AM UTC (Summer semester)  
    - cron: '0 6 1 9 *'    # September 1st at 6 AM UTC (Fall semester)
  
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      max_professors:
        description: 'Maximum number of professors to scrape (leave empty for all)'
        required: false
        default: ''
      test_mode:
        description: 'Run in test mode (10 professors only)'
        type: boolean
        required: false
        default: false

jobs:
  update-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create data directory
      run: mkdir -p data
      
    - name: Run scraper
      run: |
        if [ "${{ github.event.inputs.test_mode }}" = "true" ]; then
          echo "Running in test mode..."
          python -m scraper.main_scraper --test --skip-reviews
        elif [ -n "${{ github.event.inputs.max_professors }}" ]; then
          echo "Running with max professors: ${{ github.event.inputs.max_professors }}"
          python -m scraper.main_scraper --max-professors ${{ github.event.inputs.max_professors }} --skip-reviews
        else
          echo "Running full scrape..."
          # For now, limit to 1000 professors to avoid timeouts
          python -m scraper.main_scraper --max-professors 1000 --skip-reviews
        fi
      
    - name: Check if data was updated
      id: check_changes
      run: |
        if [ -f "data/penn_state_professors.jsonl" ]; then
          echo "Data file exists"
          echo "data_exists=true" >> $GITHUB_OUTPUT
          
          # Count professors scraped
          PROF_COUNT=$(wc -l < data/penn_state_professors.jsonl)
          echo "professors_count=$PROF_COUNT" >> $GITHUB_OUTPUT
          echo "Scraped $PROF_COUNT professors"
        else
          echo "No data file found"
          echo "data_exists=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Commit and push changes
      if: steps.check_changes.outputs.data_exists == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add the data files
        git add data/
        git add *.log || true  # Add log files if they exist
        
        # Create commit message with timestamp and count
        TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
        PROF_COUNT="${{ steps.check_changes.outputs.professors_count }}"
        
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "🤖 Auto-update: $PROF_COUNT professors scraped on $TIMESTAMP

          Automated data update from GitHub Actions workflow.
          
          📊 Professors scraped: $PROF_COUNT
          📅 Update time: $TIMESTAMP
          🎯 Target: Penn State University (ID: 758)
          
          Generated by: .github/workflows/update-data.yml"
          
          git push
          echo "Data successfully updated and pushed!"
        fi
        
    - name: Create summary
      if: always()
      run: |
        echo "## 📊 Penn State RMP Data Update Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.check_changes.outputs.data_exists }}" = "true" ]; then
          PROF_COUNT="${{ steps.check_changes.outputs.professors_count }}"
          echo "✅ **Success!** Scraped $PROF_COUNT professors" >> $GITHUB_STEP_SUMMARY
          echo "📁 **Files updated:** \`data/penn_state_professors.jsonl\`" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Failed** - No data was scraped" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "🕐 **Completed:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
        echo "🎯 **Target:** Penn State University (School ID: 758)" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event.inputs.test_mode }}" = "true" ]; then
          echo "🧪 **Mode:** Test (limited to 10 professors)" >> $GITHUB_STEP_SUMMARY
        elif [ -n "${{ github.event.inputs.max_professors }}" ]; then
          echo "🔢 **Mode:** Limited (${{ github.event.inputs.max_professors }} professors max)" >> $GITHUB_STEP_SUMMARY
        else
          echo "🚀 **Mode:** Full scrape (up to 1000 professors)" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs-${{ github.run_number }}
        path: |
          *.log
          data/
        retention-days: 30
